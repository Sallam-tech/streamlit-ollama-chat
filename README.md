# ğŸ¤– Local LLM Chat â€” Streamlit + Ollama

This project is part of my **Generative AI Internship (Arch Technologies)**.

I built a simple chat interface using **Streamlit** that connects to a locally running LLM through **Ollama**.  
All AI processing runs **on the local machine** â€” nothing goes to the cloud.

---

## ğŸš€ Features

âœ… Chat UI built with Streamlit  
âœ… Uses Ollama local LLM (`llama3.2`)  
âœ… Conversation history  
âœ… Reset Chat button  
âœ… Safe streaming response handling  
âœ… Runs fully offline

---

## ğŸ› ï¸ Tech Stack

- Python
- Streamlit
- Ollama
- Requests (API calls)
- Llama 3.2 Model

---

## ğŸ“¦ Installation & Setup

### 1ï¸âƒ£ Install Python (if not installed)
https://www.python.org/downloads/

---

### 2ï¸âƒ£ Install Ollama

Download from:

https://ollama.com/download

Then pull the model:

```bash
ollama run llama3.2
```

(Wait for download to finish, then `/exit`.)

---

### 3ï¸âƒ£ Install dependencies

```bash
pip install streamlit requests
```

---

### 4ï¸âƒ£ Run the app

Inside the project folder, run:

```bash
python -m streamlit run app.py
```

Then open:

```
http://localhost:8501
```

---

## ğŸ§  How It Works

1ï¸âƒ£ User types question in Streamlit  
2ï¸âƒ£ Python sends POST request to:

```
http://localhost:11434/api/generate
```

3ï¸âƒ£ Ollama generates answer locally  
4ï¸âƒ£ Response is displayed in the chat interface

No external servers. Fully private.

---

## ğŸ“¸ Screenshots

> (Add screenshots here)

- App UI
- Sample chat
- Reset button

---

## ğŸ¥ Demo Video

A short demo video is available on LinkedIn:



---

## ğŸ“š What I Learned

âœ”ï¸ Working with local LLMs  
âœ”ï¸ API integration  
âœ”ï¸ Debugging streaming JSON  
âœ”ï¸ Building UI with Streamlit  
âœ”ï¸ Handling chat history state  

This project helped me understand real-world AI application workflows.

---

### ğŸ™Œ Internship

**Arch Technologies â€” Generative AI Internship**
Month 1 â€” Task 1: Streamlit + Ollama Interface
